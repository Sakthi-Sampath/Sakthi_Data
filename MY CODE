DEVELOPED PYSPARK CODE TO FIND DUPLICATIONS FOR MY PROJECT DATA:-

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

duplicate_columns = {
    "AgencyTransactions" : ["AgencyTransactionID"],
    "Agents" : ["AgentID"],
    "CodeTypes" : ["CodeTypeID"],
    "Codes" : ["CodeID"],
    "Coverages" : ["CoverageID"],
    "Custom__ParentAgency" : ["ParentAgencyID"],
    "MajorCoverages" : ["MajorCoverageID"]
}

def check_duplicates_status(df, table_name, duplicate_columns):
    duplicates = df.groupBy(duplicate_columns).count().filter("count > 1")
    if duplicates.count() > 1:
        return "Yes"
    else:
        return "No"

duplicate_status = []
for table_name, cols in duplicate_columns.items():
    for col_name in cols:
        status = check_duplicates_status(eval(f"df_{table_name}"), table_name, col_name)
        duplicate_status.append((table_name, col_name, status))

df_duplicate_status = spark.createDataFrame(duplicate_status, ["TableName", "ColumnName", "Duplicates"])

display(df_duplicate_status)
